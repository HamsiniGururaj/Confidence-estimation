{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9727b1d7-1d81-4878-8f42-ba4ca9f12f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import metrics\n",
    "import resnet\n",
    "\n",
    "# import crl_utils\n",
    "\n",
    "# import train\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ed1d9e-c3f3-4e94-bf80-c4ceed168c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    save_folder = 'checkpoints/cifar10'  # or any path where you want to save models\n",
    "    gpus = '0'                           # doesn't affect CPU, but required if referenced\n",
    "    label_size = 2500                    # choose based on your experiment (e.g. 1000, 2000, 4000)\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d969a5-20cc-431f-be3f-8b0a6fc52f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(\"cifar10_data\", exist_ok=True)\n",
    "\n",
    "\n",
    "#import argparse\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('--save_folder', type=str, required=True )\n",
    "#parser.add_argument('--gpus', type=str, required=True )\n",
    "#parser.add_argument('--label_size', type = int, required=True)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class cifar10_dataset_labeled(Dataset):\n",
    "    def __init__(self, img_file, label_file,  train=True, train_transforms=None, test_transforms = None):  \n",
    "        super(cifar10_dataset_labeled, self).__init__()  \n",
    "        \n",
    "        self.img_file = np.load(img_file) \n",
    "        self.label_file = np.load(label_file)\n",
    "\n",
    "        self.train_transforms = train_transforms\n",
    "        self.test_transforms = test_transforms\n",
    "        self.train = train\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        if self.train == True:\n",
    "            return args.label_size\n",
    "        else:\n",
    "            return 10000\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.train == True:\n",
    "            \n",
    "            label = self.label_file[idx]\n",
    "            # label = torch.tensor(label)\n",
    "            image = Image.fromarray( self.img_file[idx].reshape(3,32,32).transpose(1,2,0) )\n",
    "            image = self.train_transforms(image) \n",
    "            \n",
    "        else:\n",
    "            label = self.label_file[idx]\n",
    "            # label = torch.tensor(label)\n",
    "            image = Image.fromarray( self.img_file[idx].reshape(3,32,32).transpose(1,2,0) )\n",
    "            image = self.test_transforms(image)\n",
    "            \n",
    "        return image, label, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0ea63f-7fc0-4220-a9e0-f6132f9afefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifar10_dataset_unlabeled(Dataset):\n",
    "    def __init__(self, img_file, label_file, train_transforms=None):  \n",
    "        super(cifar10_dataset_unlabeled, self).__init__()  \n",
    "        \n",
    "        self.img_file = np.load(img_file) \n",
    "        self.label_file = np.load(label_file)\n",
    "        self.train_transforms = train_transforms\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "\n",
    "        return 50000 - args.label_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "            \n",
    "        label = self.label_file[idx]\n",
    "        # label = torch.tensor(label)\n",
    "        image = Image.fromarray( self.img_file[idx].reshape(3,32,32).transpose(1,2,0) )\n",
    "        image = self.train_transforms(image) \n",
    "            \n",
    "        return image, label, idx + args.label_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768157d3-d565-4f43-bcb9-89ad01162756",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e069ed-468c-4e60-a453-1c72179027cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.cifar10_dataset_labeled at 0x7f789e91f620>,\n",
       " <__main__.cifar10_dataset_labeled at 0x7f7844be9d10>,\n",
       " <__main__.cifar10_dataset_unlabeled at 0x7f7840bee7b0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = cifar10_dataset_labeled(\n",
    "    'cifar10_dataset/img_labeled_' + str(args.label_size) + '.npy',\n",
    "    'cifar10_dataset/ann_labeled_' + str(args.label_size) + '.npy',\n",
    "    train=True, train_transforms=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=10)\n",
    "\n",
    "testset = cifar10_dataset_labeled(\n",
    "     'cifar10_dataset/img_test.npy',\n",
    "     'cifar10_dataset/ann_test.npy',\n",
    "     train=False, test_transforms=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=128, shuffle=False, num_workers=10)\n",
    "\n",
    "unlabeled_trainset = cifar10_dataset_unlabeled(\n",
    "    'cifar10_dataset/img_unlabeled_'+ str(args.label_size) + '.npy',\n",
    "    'cifar10_dataset/ann_unlabeled_'+ str(args.label_size) + '.npy',\n",
    "    train_transforms=transform_train)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(\n",
    "    unlabeled_trainset, batch_size=128, shuffle=True, num_workers=10)\n",
    "\n",
    "trainset, testset, unlabeled_trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8549a3e8-c046-40fb-879a-53a83ddd22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class History_consistent(object):\n",
    "    def __init__(self, n_data):\n",
    "        self.consistency = np.zeros((n_data))\n",
    "\n",
    "        self.temp = np.zeros((n_data)) + 100\n",
    "        self.max_correctness = 1\n",
    "\n",
    "    # correctness update\n",
    "    def consistency_update(self, data_idx, output):\n",
    "        probs = torch.nn.functional.softmax(output, dim=1)\n",
    "        confidence, classes = probs.max(dim=1)\n",
    "        data_idx = data_idx.cpu().numpy()\n",
    "\n",
    "        self.consistency[data_idx] += (classes.cpu().numpy() == self.temp[data_idx]).astype(int)\n",
    "        self.temp[data_idx] = classes.cpu().numpy()\n",
    "\n",
    "    # max correctness update\n",
    "    # get target & margin\n",
    "    def get_target_margin(self, data_idx1):\n",
    "\n",
    "        out = self.consistency[data_idx1]\n",
    "\n",
    "        return (out - np.min(out)) / ( np.max(out) - np.min(out) + 1e-10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e511f689-013e-4c36-96dc-ca94d4ff6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class History_correct(object):\n",
    "    def __init__(self, n_data):\n",
    "        self.correctness = np.zeros((n_data))\n",
    "\n",
    "        self.max_correctness = 1\n",
    "\n",
    "    # correctness update\n",
    "    def correctness_update(self, data_idx, correctness):\n",
    "\n",
    "\n",
    "        data_idx = data_idx.cpu().numpy()\n",
    "\n",
    "        self.correctness[data_idx] += correctness.cpu().numpy()\n",
    "\n",
    "    # max correctness update\n",
    "    # get target & margin\n",
    "    def get_target_margin(self, data_idx1):\n",
    "\n",
    "        out = self.correctness[data_idx1]\n",
    "\n",
    "        return (out - np.min(out)) / ( np.max(out) - np.min(out) + 1e-10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28b02ee-7e62-4c76-b091-baa6b0d25cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRL_loss_base(con, score, iter_i):\n",
    "    # print(len(con))\n",
    "    permi= torch.randperm(len(con))\n",
    "    all_CRL_loss = 0\n",
    "\n",
    "    con = con[ permi ]\n",
    "    score = score[ permi ]    \n",
    "    for i in range(iter_i):\n",
    "        \n",
    "        \n",
    "        \n",
    "        con1 = torch.roll(con, i + 1)\n",
    "        score1 = torch.roll(score, i + 1)\n",
    "        # print(score1 == score)\n",
    "        for_see = torch.nn.functional.relu(-torch.sign(con1 - con) * \n",
    "                                                    (score1 - score) + torch.abs( con1 - con ))\n",
    "        \n",
    "        all_CRL_loss = all_CRL_loss + torch.sum(for_see) / len(con)\n",
    "\n",
    "    return all_CRL_loss/iter_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4098ead-2b15-4b52-9992-a1ed36e942df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cumu(history, labelhistory,labelloader, unlabelloader,model):\n",
    "\n",
    "    for img, tar, idx in labelloader:\n",
    "        img, tar= img.to(device), tar.to(device)\n",
    "        out = model(img)        \n",
    "        history.consistency_update(idx, out)\n",
    "        prec, correct = utils.accuracy(out, tar)\n",
    "        labelhistory.correctness_update(idx, correct)\n",
    "    \n",
    "    for img, tar, idx in unlabelloader:\n",
    "        img, tar= img.to(device), tar.to(device)\n",
    "        out = model(img)        \n",
    "        history.consistency_update(idx, out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2278f92-748b-4f25-952d-cbf93853397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(labeledloader, unlabeledloader, model, criterion_cls, optimizer, epoch, history, labelhistory, logger):\n",
    "    batch_time = utils.AverageMeter()\n",
    "    data_time = utils.AverageMeter()\n",
    "    total_losses = utils.AverageMeter()\n",
    "    top1 = utils.AverageMeter()\n",
    "    cls_losses = utils.AverageMeter()\n",
    "    ranking_losses = utils.AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    print(total_losses)\n",
    "\n",
    "    model.train()\n",
    "    combine_cumu(history,labelhistory, labeledloader, unlabeledloader, model)\n",
    "\n",
    "    dataloader_iterator = iter(labeledloader)\n",
    "    for i, (uninput, untarget, unidx) in enumerate(unlabeledloader):\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        try:\n",
    "            input, target, idx = next(dataloader_iterator)\n",
    "        except StopIteration:\n",
    "            dataloader_iterator = iter(labeledloader)\n",
    "            input, target, idx = next(dataloader_iterator)\n",
    "            combine_cumu(history,labelhistory, labeledloader, unlabeledloader, model)\n",
    "\n",
    "\n",
    "\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        uninput = uninput.to(device)\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        unoutput = model(uninput)\n",
    "\n",
    "        # compute ranking target value normalize (0 ~ 1) range\n",
    "        # max(softmax)\n",
    "\n",
    "        conf = F.softmax(output, dim=1)\n",
    "        confidence, _ = conf.max(dim=1)\n",
    "        unconfidence,_ = F.softmax(unoutput, dim=1).max(dim=1)\n",
    "\n",
    "        finconfidence = torch.cat((confidence,unconfidence))\n",
    "        rank_target_corr = labelhistory.get_target_margin(idx)\n",
    "        rank_target = history.get_target_margin(idx)\n",
    "        unrank_target = history.get_target_margin(unidx)\n",
    "\n",
    "        finrank_target = np.concatenate( (rank_target, unrank_target) )\n",
    "        rank_target_corr = torch.tensor(rank_target_corr).to(device)\n",
    "        finrank_target = torch.tensor(finrank_target).to(device)\n",
    "        # print(rank_target)\n",
    "        # ranking loss\n",
    "        ranking_loss = CRL_loss_base(finrank_target, finconfidence, 1)\n",
    "        rank_corr = CRL_loss_base(rank_target_corr, confidence, 1)\n",
    "        # print(ranking_loss)\n",
    "        # total loss\n",
    "        cls_loss = criterion_cls(output, target)\n",
    "\n",
    "        loss = cls_loss + 0.5 * ranking_loss + 0.5 * rank_corr\n",
    "\n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record loss and accuracy\n",
    "        prec, correct = utils.accuracy(output, target)\n",
    "        total_losses.update(loss.item(), input.size(0))\n",
    "        cls_losses.update(cls_loss.item(), input.size(0))\n",
    "        ranking_losses.update( 0.5 * ranking_loss.item() + 0.5 * rank_corr.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}\\t\"\n",
    "                  f\"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                  f\"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                  f\"Loss {loss.item():.4f}\\t\"\n",
    "                  f\"Cls Loss {cls_loss.item():.4f}\\t\"\n",
    "                  f\"Rank Loss {ranking_loss.item():.4f}\\t\"\n",
    "                  f\"Prec {top1.val:.2f}% ({top1.avg:.2f})%\")\n",
    "\n",
    "        # correctness count update\n",
    "        # history.correctness_update(idx, correct)\n",
    "    # max correctness update\n",
    "\n",
    "    logger.write([epoch, total_losses.avg, cls_losses.avg, ranking_losses.avg, top1.avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93d1135-1090-414e-acef-a2fcfb285f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def calc_aurc_eaurc(softmax, correct):\n",
    "    softmax = np.array(softmax)\n",
    "    correctness = np.array(correct)\n",
    "    softmax_max = np.max(softmax, 1)\n",
    "\n",
    "    sort_values = sorted(zip(softmax_max[:], correctness[:]), key=lambda x:x[0], reverse=True)\n",
    "    sort_softmax_max, sort_correctness = zip(*sort_values)\n",
    "    risk_li, coverage_li = coverage_risk(sort_softmax_max, sort_correctness)\n",
    "    aurc, eaurc = aurc_eaurc(risk_li)\n",
    "\n",
    "    return aurc, eaurc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04242245-0cb3-47ff-ad04-dacaf5cfed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUPR ERROR\n",
    "def calc_fpr_aupr(softmax, correct):\n",
    "    softmax = np.array(softmax)\n",
    "    correctness = np.array(correct)\n",
    "    softmax_max = np.max(softmax, 1)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(correctness, softmax_max)\n",
    "    idx_tpr_95 = np.argmin(np.abs(tpr - 0.95))\n",
    "    fpr_in_tpr_95 = fpr[idx_tpr_95]\n",
    "\n",
    "    aupr_err = metrics.average_precision_score(-1 * correctness + 1, -1 * softmax_max)\n",
    "\n",
    "    print(\"AUPR {0:.2f}\".format(aupr_err*100))\n",
    "    print('FPR {0:.2f}'.format(fpr_in_tpr_95*100))\n",
    "\n",
    "    return aupr_err, fpr_in_tpr_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4f01a1-24bc-40b5-991f-6db6b13026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECE\n",
    "def calc_ece(softmax, label, bins=15):\n",
    "    bin_boundaries = torch.linspace(0, 1, bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    softmax = torch.tensor(softmax)\n",
    "    labels = torch.tensor(label)\n",
    "\n",
    "    softmax_max, predictions = torch.max(softmax, 1)\n",
    "    correctness = predictions.eq(labels)\n",
    "\n",
    "    ece = torch.zeros(1)\n",
    "\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = softmax_max.gt(bin_lower.item()) * softmax_max.le(bin_upper.item())\n",
    "        prop_in_bin = in_bin.float().mean()\n",
    "\n",
    "        if prop_in_bin.item() > 0.0:\n",
    "            accuracy_in_bin = correctness[in_bin].float().mean()\n",
    "            avg_confidence_in_bin = softmax_max[in_bin].mean()\n",
    "\n",
    "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "    print(\"ECE {0:.2f} \".format(ece.item()*100))\n",
    "\n",
    "    return ece.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa25976e-8a77-47f5-9efb-844d06efeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL & Brier Score\n",
    "def calc_nll_brier(softmax, logit, label, label_onehot):\n",
    "    brier_score = np.mean(np.sum((softmax - label_onehot) ** 2, axis=1))\n",
    "\n",
    "    logit = torch.tensor(logit, dtype=torch.float)\n",
    "    label = torch.tensor(label, dtype=torch.int)\n",
    "    logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    log_softmax = logsoftmax(logit)\n",
    "    nll = calc_nll(log_softmax, label)\n",
    "\n",
    "    print(\"NLL {0:.2f} \".format(nll.item()*10))\n",
    "    print('Brier {0:.2f}'.format(brier_score*100))\n",
    "\n",
    "    return nll.item(), brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203b616a-cea1-42e7-8d63-61b653e46971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc NLL\n",
    "def calc_nll(log_softmax, label):\n",
    "    out = torch.zeros_like(label, dtype=torch.float)\n",
    "    for i in range(len(label)):\n",
    "        out[i] = log_softmax[i][label[i]]\n",
    "\n",
    "    return -out.sum()/len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f79230a7-e7fe-46a0-9adb-b72a64bfa36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc coverage, risk\n",
    "def coverage_risk(confidence, correctness):\n",
    "    risk_list = []\n",
    "    coverage_list = []\n",
    "    risk = 0\n",
    "    for i in range(len(confidence)):\n",
    "        coverage = (i + 1) / len(confidence)\n",
    "        coverage_list.append(coverage)\n",
    "\n",
    "        if correctness[i] == 0:\n",
    "            risk += 1\n",
    "\n",
    "        risk_list.append(risk / (i + 1))\n",
    "\n",
    "    return risk_list, coverage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6660ce9b-e7fc-43f2-9220-249a14665e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc aurc, eaurc\n",
    "def aurc_eaurc(risk_list):\n",
    "    r = risk_list[-1]\n",
    "    risk_coverage_curve_area = 0\n",
    "    optimal_risk_area = r + (1 - r) * np.log(1 - r)\n",
    "    for risk_value in risk_list:\n",
    "        risk_coverage_curve_area += risk_value * (1 / len(risk_list))\n",
    "\n",
    "    aurc = risk_coverage_curve_area\n",
    "    eaurc = risk_coverage_curve_area - optimal_risk_area\n",
    "\n",
    "    print(f\"AURC {aurc*1000:.2f}\")\n",
    "    print(f\"EAURC {eaurc*1000:.2f}\")\n",
    "\n",
    "    return aurc, eaurc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a5d94c6-9bee-48b1-b5e4-49cfc83b569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(data_loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    list_softmax = []\n",
    "    list_correct = []\n",
    "    list_logit = []\n",
    "    label_list = []\n",
    "    list_onehot = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets,idx_list in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets\n",
    "            label_list.extend(targets)\n",
    "            list_onehot.extend( F.one_hot(targets, num_classes=10).data.numpy() )\n",
    "            outputs = model(inputs)\n",
    "            list_softmax.extend(F.softmax(outputs,dim=1).cpu().data.numpy())\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            for i in outputs:\n",
    "                list_logit.append(i.cpu().data.numpy())\n",
    "            for j in range(len(pred)):\n",
    "                if pred[j] == targets[j]:\n",
    "                    cor = 1\n",
    "                else:\n",
    "                    cor = 0\n",
    "                list_correct.append(cor)\n",
    "    list_onehot = np.array(list_onehot)\n",
    "    aurc, eaurc = calc_aurc_eaurc(list_softmax, list_correct)\n",
    "    aupr, fpr = calc_fpr_aupr(list_softmax, list_correct)\n",
    "    ece = calc_ece(list_softmax, label_list, bins=15)\n",
    "    nll, brier = calc_nll_brier(list_softmax, list_logit, label_list, list_onehot)\n",
    "    return np.mean(list_correct),aurc, eaurc, aupr, fpr, ece, nll, brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276bc6d3-bec0-4668-9e19-ffb62b4af5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set GPU ID\n",
    "    #os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n",
    "    #cudnn.benchmark = True\n",
    "\n",
    "    # check save path\n",
    "    save_path = args.save_folder\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # make dataloader\n",
    "    # train_loader, test_loader = trainloader,testloader\n",
    "\n",
    "\n",
    "    num_class = 10\n",
    "\n",
    "    # set num_classes\n",
    "    model_dict = {\n",
    "        \"num_classes\": num_class,\n",
    "    }\n",
    "\n",
    "    # set model\n",
    "\n",
    "    model = resnet.resnet110(**model_dict).to(device)\n",
    "\n",
    "\n",
    "    # set criterion\n",
    "    cls_criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # set optimizer (default:sgd)\n",
    "    optimizer = optim.SGD(model.parameters(),\n",
    "                          lr=0.1,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.0001,\n",
    "                          nesterov=False)\n",
    "\n",
    "    # set scheduler\n",
    "    scheduler = MultiStepLR(optimizer,\n",
    "                            milestones=[150,250],\n",
    "                            gamma=0.1)\n",
    "\n",
    "    # make logger\n",
    "    train_logger = utils.Logger(os.path.join(save_path, 'train.log'))\n",
    "    result_logger = utils.Logger(os.path.join(save_path, 'result.log'))\n",
    "\n",
    "    # make History Class\n",
    "    correctness_history = History_correct(args.label_size)\n",
    "    \n",
    "    consistency_history = History_consistent(50000)\n",
    "\n",
    "    # start Train\n",
    "    for epoch in range(1, 2 + 1):\n",
    "        \n",
    "        train_function(trainloader,\n",
    "                    unlabeled_loader,\n",
    "                    model,\n",
    "                    cls_criterion,\n",
    "                    optimizer, \n",
    "                    epoch,\n",
    "                    consistency_history,\n",
    "                    correctness_history,\n",
    "                    train_logger,\n",
    "                    )\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                    os.path.join(save_path, 'model_' + str(epoch) + '.pth'))            \n",
    "        acc, aurc, eaurc, aupr, fpr, ece, nll, brier = calc_metrics(testloader, model)\n",
    "        print(f'Accuracy:{acc*100}')\n",
    "        # save model\n",
    "        # #if epoch == 300:\n",
    "        # torch.save(model.state_dict(),\n",
    "        #                os.path.join(save_path, 'model.pth'))\n",
    "    # finish train\n",
    "\n",
    "    # calc measure\n",
    "    acc, aurc, eaurc, aupr, fpr, ece, nll, brier = calc_metrics(testloader, model)\n",
    "    print(f'Accuracy:{acc*100}')\n",
    "    # result write\n",
    "    result_logger.write([acc, aurc*1000, eaurc*1000, aupr*100, fpr*100, ece*100, nll*10, brier*100])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c43cfdb-c594-4fe9-8d8c-047d21ad0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<utils.AverageMeter object at 0x7f7840b35950>\n",
      "Epoch: 1\tTime 71.576 (71.576)\tData 70.289 (70.289)\tLoss 2.4198\tCls Loss 2.3414\tRank Loss 0.0000\tPrec 12.50% (12.50)%\n",
      "Epoch: 1\tTime 0.740 (7.183)\tData 0.000 (6.391)\tLoss 2.3376\tCls Loss 2.2902\tRank Loss 0.0000\tPrec 6.25% (10.23)%\n",
      "Epoch: 1\tTime 0.745 (4.120)\tData 0.000 (3.348)\tLoss 2.3310\tCls Loss 2.2654\tRank Loss 0.0000\tPrec 21.88% (13.62)%\n",
      "Epoch: 1\tTime 0.809 (3.045)\tData 0.001 (2.268)\tLoss 2.1890\tCls Loss 2.1133\tRank Loss 0.0000\tPrec 20.31% (15.22)%\n",
      "Epoch: 1\tTime 75.445 (4.312)\tData 0.001 (1.715)\tLoss 2.3961\tCls Loss 2.1440\tRank Loss 0.3157\tPrec 21.88% (16.15)%\n",
      "Epoch: 1\tTime 0.818 (3.628)\tData 0.001 (1.379)\tLoss 2.2220\tCls Loss 2.0009\tRank Loss 0.3115\tPrec 25.00% (16.85)%\n",
      "Epoch: 1\tTime 0.747 (3.166)\tData 0.002 (1.154)\tLoss 2.3903\tCls Loss 2.0699\tRank Loss 0.4001\tPrec 28.12% (18.00)%\n",
      "Epoch: 1\tTime 0.750 (2.828)\tData 0.001 (0.991)\tLoss 2.4472\tCls Loss 2.1623\tRank Loss 0.3895\tPrec 23.44% (18.84)%\n",
      "Epoch: 1\tTime 76.058 (3.500)\tData 0.001 (0.869)\tLoss 2.3934\tCls Loss 2.1447\tRank Loss 0.2475\tPrec 15.62% (18.98)%\n",
      "Epoch: 1\tTime 0.810 (3.203)\tData 0.002 (0.774)\tLoss 2.1870\tCls Loss 1.9696\tRank Loss 0.2526\tPrec 29.69% (19.53)%\n",
      "Epoch: 1\tTime 0.803 (2.966)\tData 0.002 (0.697)\tLoss 2.0952\tCls Loss 1.8939\tRank Loss 0.2046\tPrec 28.12% (20.24)%\n",
      "Epoch: 1\tTime 0.801 (2.771)\tData 0.001 (0.635)\tLoss 2.2764\tCls Loss 2.0069\tRank Loss 0.2684\tPrec 20.31% (20.89)%\n",
      "Epoch: 1\tTime 75.873 (3.227)\tData 0.001 (0.582)\tLoss 2.0336\tCls Loss 1.8059\tRank Loss 0.2327\tPrec 37.50% (21.43)%\n",
      "Epoch: 1\tTime 0.814 (3.044)\tData 0.002 (0.538)\tLoss 2.1812\tCls Loss 1.8536\tRank Loss 0.2967\tPrec 31.25% (22.06)%\n",
      "Epoch: 1\tTime 0.822 (2.886)\tData 0.001 (0.500)\tLoss 2.1560\tCls Loss 1.8376\tRank Loss 0.3034\tPrec 32.81% (22.51)%\n",
      "Epoch: 1\tTime 0.808 (2.748)\tData 0.001 (0.467)\tLoss 1.9552\tCls Loss 1.7301\tRank Loss 0.2154\tPrec 31.25% (23.08)%\n",
      "Epoch: 1\tTime 76.493 (3.094)\tData 0.001 (0.438)\tLoss 2.0887\tCls Loss 1.8428\tRank Loss 0.2169\tPrec 31.25% (23.32)%\n",
      "Epoch: 1\tTime 0.810 (2.961)\tData 0.002 (0.412)\tLoss 2.0952\tCls Loss 1.8488\tRank Loss 0.2252\tPrec 31.25% (23.61)%\n",
      "Epoch: 1\tTime 0.745 (2.840)\tData 0.002 (0.390)\tLoss 2.1411\tCls Loss 1.9344\tRank Loss 0.2668\tPrec 28.12% (24.08)%\n",
      "Epoch: 1\tTime 0.811 (2.732)\tData 0.002 (0.369)\tLoss 1.9354\tCls Loss 1.6804\tRank Loss 0.2320\tPrec 35.94% (24.66)%\n",
      "Epoch: 1\tTime 76.625 (3.013)\tData 0.001 (0.351)\tLoss 1.9614\tCls Loss 1.7706\tRank Loss 0.2107\tPrec 31.25% (24.92)%\n",
      "Epoch: 1\tTime 0.816 (2.908)\tData 0.002 (0.335)\tLoss 2.0669\tCls Loss 1.7924\tRank Loss 0.2259\tPrec 31.25% (25.26)%\n",
      "Epoch: 1\tTime 0.810 (2.813)\tData 0.002 (0.319)\tLoss 1.9406\tCls Loss 1.6790\tRank Loss 0.2174\tPrec 31.25% (25.58)%\n",
      "Epoch: 1\tTime 0.809 (2.727)\tData 0.002 (0.306)\tLoss 2.0468\tCls Loss 1.7499\tRank Loss 0.2795\tPrec 46.88% (25.91)%\n",
      "Epoch: 1\tTime 75.609 (2.957)\tData 0.001 (0.293)\tLoss 1.8856\tCls Loss 1.6728\tRank Loss 0.2302\tPrec 39.06% (26.13)%\n",
      "Epoch: 1\tTime 0.747 (2.869)\tData 0.001 (0.281)\tLoss 2.0110\tCls Loss 1.7444\tRank Loss 0.2600\tPrec 37.50% (26.39)%\n",
      "Epoch: 1\tTime 0.810 (2.790)\tData 0.002 (0.271)\tLoss 2.3350\tCls Loss 2.0787\tRank Loss 0.2428\tPrec 26.56% (26.60)%\n",
      "Epoch: 1\tTime 0.748 (2.715)\tData 0.002 (0.261)\tLoss 1.8090\tCls Loss 1.6015\tRank Loss 0.2139\tPrec 34.38% (26.81)%\n",
      "Epoch: 1\tTime 75.715 (2.911)\tData 0.001 (0.252)\tLoss 1.8924\tCls Loss 1.6835\tRank Loss 0.2342\tPrec 34.38% (27.12)%\n",
      "Epoch: 1\tTime 0.820 (2.839)\tData 0.002 (0.243)\tLoss 1.9880\tCls Loss 1.7706\tRank Loss 0.2263\tPrec 42.19% (27.28)%\n",
      "Epoch: 1\tTime 0.811 (2.772)\tData 0.002 (0.235)\tLoss 1.9715\tCls Loss 1.7428\tRank Loss 0.2280\tPrec 28.12% (27.51)%\n",
      "Epoch: 1\tTime 0.810 (2.709)\tData 0.002 (0.227)\tLoss 1.9507\tCls Loss 1.7102\tRank Loss 0.2238\tPrec 35.94% (27.80)%\n",
      "Epoch: 1\tTime 77.784 (2.889)\tData 0.002 (0.220)\tLoss 1.9445\tCls Loss 1.6528\tRank Loss 0.2409\tPrec 35.94% (28.10)%\n",
      "Epoch: 1\tTime 0.812 (2.826)\tData 0.002 (0.214)\tLoss 2.0955\tCls Loss 1.8437\tRank Loss 0.2268\tPrec 35.94% (28.35)%\n",
      "Epoch: 1\tTime 0.835 (2.767)\tData 0.002 (0.208)\tLoss 1.9930\tCls Loss 1.7191\tRank Loss 0.2240\tPrec 37.50% (28.72)%\n",
      "Epoch: 1\tTime 0.834 (2.712)\tData 0.001 (0.202)\tLoss 2.1074\tCls Loss 1.8495\tRank Loss 0.2402\tPrec 29.69% (28.88)%\n",
      "Epoch: 1\tTime 76.964 (2.870)\tData 0.001 (0.196)\tLoss 2.0803\tCls Loss 1.8643\tRank Loss 0.1838\tPrec 25.00% (29.02)%\n",
      "Epoch: 1\tTime 0.755 (2.813)\tData 0.001 (0.191)\tLoss 2.0994\tCls Loss 1.8309\tRank Loss 0.2417\tPrec 28.12% (29.09)%\n",
      "AURC 618.14\n",
      "EAURC 280.77\n",
      "AUPR 77.52\n",
      "FPR 89.01\n",
      "ECE 11.01 \n",
      "NLL 20.19 \n",
      "Brier 83.28\n",
      "Accuracy:30.12\n",
      "<utils.AverageMeter object at 0x7f784541aa50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68555/1522670346.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  softmax = torch.tensor(softmax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\tTime 77.581 (77.581)\tData 76.304 (76.304)\tLoss 1.9041\tCls Loss 1.6510\tRank Loss 0.1966\tPrec 45.31% (45.31)%\n",
      "Epoch: 2\tTime 0.818 (7.804)\tData 0.000 (6.939)\tLoss 2.0409\tCls Loss 1.7765\tRank Loss 0.2068\tPrec 34.38% (34.09)%\n",
      "Epoch: 2\tTime 0.822 (4.478)\tData 0.000 (3.635)\tLoss 2.0202\tCls Loss 1.7590\tRank Loss 0.2028\tPrec 42.19% (35.12)%\n",
      "Epoch: 2\tTime 0.782 (3.286)\tData 0.001 (2.463)\tLoss 1.9978\tCls Loss 1.7812\tRank Loss 0.1933\tPrec 35.94% (35.33)%\n",
      "Epoch: 2\tTime 75.545 (4.501)\tData 0.002 (1.862)\tLoss 1.9303\tCls Loss 1.6859\tRank Loss 0.2057\tPrec 32.81% (35.92)%\n",
      "Epoch: 2\tTime 0.836 (3.781)\tData 0.002 (1.498)\tLoss 1.9363\tCls Loss 1.7258\tRank Loss 0.2024\tPrec 28.12% (35.36)%\n",
      "Epoch: 2\tTime 0.820 (3.295)\tData 0.002 (1.252)\tLoss 1.9049\tCls Loss 1.6024\tRank Loss 0.2320\tPrec 34.38% (35.67)%\n",
      "Epoch: 2\tTime 0.828 (2.947)\tData 0.002 (1.076)\tLoss 2.2194\tCls Loss 1.9622\tRank Loss 0.2305\tPrec 21.88% (35.86)%\n",
      "Epoch: 2\tTime 78.602 (3.643)\tData 0.001 (0.943)\tLoss 1.8867\tCls Loss 1.6724\tRank Loss 0.1974\tPrec 42.19% (36.10)%\n",
      "Epoch: 2\tTime 0.752 (3.326)\tData 0.001 (0.840)\tLoss 2.1047\tCls Loss 1.8241\tRank Loss 0.2275\tPrec 26.56% (35.92)%\n",
      "Epoch: 2\tTime 0.816 (3.072)\tData 0.002 (0.757)\tLoss 1.9639\tCls Loss 1.7096\tRank Loss 0.2416\tPrec 35.94% (36.22)%\n",
      "Epoch: 2\tTime 0.819 (2.869)\tData 0.002 (0.689)\tLoss 2.0260\tCls Loss 1.8130\tRank Loss 0.1756\tPrec 32.81% (36.44)%\n",
      "Epoch: 2\tTime 77.821 (3.334)\tData 0.002 (0.632)\tLoss 2.0933\tCls Loss 1.8615\tRank Loss 0.2203\tPrec 28.12% (36.52)%\n",
      "Epoch: 2\tTime 0.834 (3.143)\tData 0.002 (0.584)\tLoss 1.5870\tCls Loss 1.3598\tRank Loss 0.1602\tPrec 53.12% (36.90)%\n",
      "Epoch: 2\tTime 0.819 (2.978)\tData 0.002 (0.543)\tLoss 1.8132\tCls Loss 1.5222\tRank Loss 0.2019\tPrec 39.06% (37.12)%\n",
      "Epoch: 2\tTime 0.821 (2.835)\tData 0.002 (0.507)\tLoss 1.8229\tCls Loss 1.5469\tRank Loss 0.2258\tPrec 46.88% (37.49)%\n",
      "Epoch: 2\tTime 77.067 (3.183)\tData 0.001 (0.475)\tLoss 1.8303\tCls Loss 1.6070\tRank Loss 0.2131\tPrec 45.31% (37.83)%\n",
      "Epoch: 2\tTime 0.753 (3.041)\tData 0.001 (0.448)\tLoss 1.6788\tCls Loss 1.4417\tRank Loss 0.1989\tPrec 50.00% (38.00)%\n",
      "Epoch: 2\tTime 0.753 (2.914)\tData 0.002 (0.423)\tLoss 1.8420\tCls Loss 1.5703\tRank Loss 0.2121\tPrec 42.19% (38.28)%\n",
      "Epoch: 2\tTime 0.753 (2.801)\tData 0.001 (0.401)\tLoss 1.9216\tCls Loss 1.6976\tRank Loss 0.2253\tPrec 32.81% (38.39)%\n",
      "Epoch: 2\tTime 76.058 (3.076)\tData 0.002 (0.381)\tLoss 1.6713\tCls Loss 1.4575\tRank Loss 0.1418\tPrec 48.44% (38.57)%\n",
      "Epoch: 2\tTime 0.776 (2.967)\tData 0.001 (0.363)\tLoss 1.6545\tCls Loss 1.4434\tRank Loss 0.2184\tPrec 43.75% (38.94)%\n",
      "Epoch: 2\tTime 0.777 (2.868)\tData 0.001 (0.347)\tLoss 1.8530\tCls Loss 1.6490\tRank Loss 0.1798\tPrec 42.19% (39.11)%\n",
      "Epoch: 2\tTime 0.796 (2.779)\tData 0.001 (0.332)\tLoss 1.8419\tCls Loss 1.6045\tRank Loss 0.1563\tPrec 39.06% (39.26)%\n",
      "Epoch: 2\tTime 78.770 (3.021)\tData 0.001 (0.318)\tLoss 1.7329\tCls Loss 1.5029\tRank Loss 0.2213\tPrec 43.75% (39.50)%\n",
      "Epoch: 2\tTime 0.817 (2.932)\tData 0.001 (0.305)\tLoss 1.5362\tCls Loss 1.3454\tRank Loss 0.1804\tPrec 57.81% (39.84)%\n",
      "Epoch: 2\tTime 0.774 (2.850)\tData 0.001 (0.294)\tLoss 1.6738\tCls Loss 1.4113\tRank Loss 0.1864\tPrec 39.06% (40.08)%\n",
      "Epoch: 2\tTime 0.851 (2.775)\tData 0.002 (0.283)\tLoss 1.7069\tCls Loss 1.4804\tRank Loss 0.2101\tPrec 43.75% (40.34)%\n",
      "Epoch: 2\tTime 77.647 (2.979)\tData 0.002 (0.273)\tLoss 1.6591\tCls Loss 1.3939\tRank Loss 0.2096\tPrec 43.75% (40.53)%\n",
      "Epoch: 2\tTime 0.784 (2.903)\tData 0.001 (0.264)\tLoss 1.8291\tCls Loss 1.5989\tRank Loss 0.1724\tPrec 40.62% (40.61)%\n",
      "Epoch: 2\tTime 0.774 (2.833)\tData 0.001 (0.255)\tLoss 1.7442\tCls Loss 1.5090\tRank Loss 0.2220\tPrec 42.19% (40.79)%\n",
      "Epoch: 2\tTime 0.814 (2.767)\tData 0.002 (0.247)\tLoss 1.7331\tCls Loss 1.4838\tRank Loss 0.1952\tPrec 48.44% (40.96)%\n",
      "Epoch: 2\tTime 78.356 (2.946)\tData 0.002 (0.239)\tLoss 1.7994\tCls Loss 1.5896\tRank Loss 0.2200\tPrec 35.94% (41.12)%\n",
      "Epoch: 2\tTime 0.791 (2.881)\tData 0.001 (0.232)\tLoss 1.8104\tCls Loss 1.5497\tRank Loss 0.1920\tPrec 42.19% (41.24)%\n",
      "Epoch: 2\tTime 0.848 (2.821)\tData 0.002 (0.225)\tLoss 1.4745\tCls Loss 1.2329\tRank Loss 0.2076\tPrec 53.12% (41.46)%\n",
      "Epoch: 2\tTime 0.782 (2.763)\tData 0.001 (0.219)\tLoss 1.6273\tCls Loss 1.4069\tRank Loss 0.1721\tPrec 46.88% (41.71)%\n",
      "Epoch: 2\tTime 77.537 (2.920)\tData 0.001 (0.213)\tLoss 1.5773\tCls Loss 1.3314\tRank Loss 0.2025\tPrec 51.56% (41.87)%\n",
      "Epoch: 2\tTime 0.776 (2.863)\tData 0.001 (0.207)\tLoss 1.6976\tCls Loss 1.4771\tRank Loss 0.1812\tPrec 48.44% (41.93)%\n",
      "AURC 390.09\n",
      "EAURC 195.72\n",
      "AUPR 71.30\n",
      "FPR 85.52\n",
      "ECE 11.53 \n",
      "NLL 15.82 \n",
      "Brier 71.03\n",
      "Accuracy:44.54\n",
      "<utils.AverageMeter object at 0x7f78c438d4a0>\n",
      "Epoch: 3\tTime 79.012 (79.012)\tData 77.780 (77.780)\tLoss 1.5787\tCls Loss 1.3324\tRank Loss 0.1973\tPrec 50.00% (50.00)%\n",
      "Epoch: 3\tTime 0.837 (7.937)\tData 0.001 (7.072)\tLoss 1.5262\tCls Loss 1.3013\tRank Loss 0.1941\tPrec 45.31% (49.57)%\n",
      "Epoch: 3\tTime 0.845 (4.557)\tData 0.000 (3.705)\tLoss 1.7216\tCls Loss 1.4600\tRank Loss 0.2310\tPrec 50.00% (48.51)%\n",
      "Epoch: 3\tTime 0.783 (3.345)\tData 0.001 (2.510)\tLoss 1.4111\tCls Loss 1.2030\tRank Loss 0.1809\tPrec 54.69% (49.04)%\n",
      "Epoch: 3\tTime 76.785 (4.568)\tData 0.001 (1.898)\tLoss 1.5289\tCls Loss 1.3029\tRank Loss 0.1871\tPrec 46.88% (48.91)%\n",
      "Epoch: 3\tTime 0.844 (3.832)\tData 0.002 (1.527)\tLoss 1.4494\tCls Loss 1.1919\tRank Loss 0.2147\tPrec 59.38% (48.97)%\n",
      "Epoch: 3\tTime 0.839 (3.342)\tData 0.002 (1.277)\tLoss 1.6006\tCls Loss 1.3220\tRank Loss 0.2202\tPrec 46.88% (48.54)%\n",
      "Epoch: 3\tTime 0.843 (2.990)\tData 0.002 (1.097)\tLoss 1.5935\tCls Loss 1.3681\tRank Loss 0.1847\tPrec 45.31% (48.42)%\n",
      "Epoch: 3\tTime 78.996 (3.687)\tData 0.002 (0.962)\tLoss 1.5207\tCls Loss 1.2533\tRank Loss 0.2363\tPrec 60.94% (48.58)%\n",
      "Epoch: 3\tTime 0.789 (3.371)\tData 0.001 (0.856)\tLoss 1.4857\tCls Loss 1.2746\tRank Loss 0.1805\tPrec 50.00% (48.88)%\n",
      "Epoch: 3\tTime 0.779 (3.117)\tData 0.001 (0.772)\tLoss 1.3418\tCls Loss 1.1497\tRank Loss 0.2079\tPrec 65.62% (49.42)%\n",
      "Epoch: 3\tTime 0.776 (2.909)\tData 0.001 (0.702)\tLoss 1.3222\tCls Loss 1.0823\tRank Loss 0.2251\tPrec 64.06% (49.48)%\n",
      "Epoch: 3\tTime 78.376 (3.375)\tData 0.002 (0.644)\tLoss 1.5939\tCls Loss 1.3340\tRank Loss 0.2145\tPrec 42.19% (49.39)%\n",
      "Epoch: 3\tTime 0.851 (3.182)\tData 0.001 (0.595)\tLoss 1.5683\tCls Loss 1.2919\tRank Loss 0.1999\tPrec 51.56% (49.12)%\n",
      "Epoch: 3\tTime 0.858 (3.017)\tData 0.002 (0.553)\tLoss 1.6012\tCls Loss 1.3730\tRank Loss 0.1847\tPrec 50.00% (49.39)%\n",
      "Epoch: 3\tTime 0.783 (2.872)\tData 0.001 (0.517)\tLoss 1.5658\tCls Loss 1.3471\tRank Loss 0.1840\tPrec 45.31% (49.44)%\n",
      "Epoch: 3\tTime 75.777 (3.208)\tData 0.001 (0.485)\tLoss 1.4341\tCls Loss 1.1825\tRank Loss 0.2082\tPrec 54.69% (49.71)%\n",
      "Epoch: 3\tTime 0.828 (3.069)\tData 0.002 (0.456)\tLoss 1.3023\tCls Loss 1.0817\tRank Loss 0.1680\tPrec 64.06% (49.87)%\n",
      "Epoch: 3\tTime 0.821 (2.945)\tData 0.001 (0.431)\tLoss 1.6536\tCls Loss 1.4431\tRank Loss 0.1517\tPrec 48.44% (50.23)%\n",
      "Epoch: 3\tTime 0.775 (2.832)\tData 0.001 (0.409)\tLoss 1.5827\tCls Loss 1.3485\tRank Loss 0.2009\tPrec 42.19% (50.35)%\n",
      "Epoch: 3\tTime 75.429 (3.100)\tData 0.001 (0.388)\tLoss 1.2526\tCls Loss 1.0377\tRank Loss 0.1596\tPrec 64.06% (50.86)%\n",
      "Epoch: 3\tTime 0.784 (2.989)\tData 0.002 (0.370)\tLoss 1.5380\tCls Loss 1.2845\tRank Loss 0.2059\tPrec 56.25% (50.92)%\n",
      "Epoch: 3\tTime 0.755 (2.888)\tData 0.001 (0.353)\tLoss 1.5224\tCls Loss 1.2597\tRank Loss 0.2252\tPrec 54.69% (50.98)%\n",
      "Epoch: 3\tTime 0.828 (2.796)\tData 0.002 (0.338)\tLoss 1.5073\tCls Loss 1.2757\tRank Loss 0.1843\tPrec 50.00% (51.21)%\n",
      "Epoch: 3\tTime 77.398 (3.031)\tData 0.001 (0.324)\tLoss 1.3571\tCls Loss 1.1371\tRank Loss 0.1781\tPrec 56.25% (51.56)%\n",
      "Epoch: 3\tTime 0.768 (2.941)\tData 0.001 (0.311)\tLoss 1.4850\tCls Loss 1.2576\tRank Loss 0.2047\tPrec 54.69% (51.78)%\n",
      "Epoch: 3\tTime 0.826 (2.860)\tData 0.002 (0.299)\tLoss 1.4672\tCls Loss 1.2516\tRank Loss 0.1963\tPrec 59.38% (51.93)%\n",
      "Epoch: 3\tTime 0.823 (2.785)\tData 0.002 (0.288)\tLoss 1.5833\tCls Loss 1.3389\tRank Loss 0.2278\tPrec 48.44% (51.98)%\n",
      "Epoch: 3\tTime 77.832 (2.988)\tData 0.002 (0.278)\tLoss 1.7940\tCls Loss 1.5624\tRank Loss 0.1994\tPrec 43.75% (52.04)%\n",
      "Epoch: 3\tTime 0.714 (3.291)\tData 0.001 (0.269)\tLoss 1.4862\tCls Loss 1.2330\tRank Loss 0.2277\tPrec 60.94% (52.05)%\n",
      "Epoch: 3\tTime 0.780 (3.207)\tData 0.001 (0.260)\tLoss 1.3461\tCls Loss 1.1281\tRank Loss 0.1887\tPrec 62.50% (52.26)%\n",
      "Epoch: 3\tTime 0.797 (3.131)\tData 0.001 (0.252)\tLoss 1.4173\tCls Loss 1.2194\tRank Loss 0.1912\tPrec 50.00% (52.40)%\n",
      "Epoch: 3\tTime 79.905 (3.304)\tData 0.001 (0.244)\tLoss 1.2864\tCls Loss 1.1085\tRank Loss 0.1644\tPrec 51.56% (52.46)%\n",
      "Epoch: 3\tTime 0.879 (3.229)\tData 0.001 (0.237)\tLoss 1.4566\tCls Loss 1.2344\tRank Loss 0.1942\tPrec 54.69% (52.55)%\n",
      "Epoch: 3\tTime 0.798 (3.158)\tData 0.001 (0.230)\tLoss 1.1859\tCls Loss 0.9704\tRank Loss 0.1719\tPrec 67.19% (52.82)%\n",
      "Epoch: 3\tTime 0.785 (3.091)\tData 0.001 (0.223)\tLoss 1.3728\tCls Loss 1.1382\tRank Loss 0.2271\tPrec 59.38% (52.96)%\n",
      "Epoch: 3\tTime 78.485 (3.243)\tData 0.002 (0.217)\tLoss 1.1518\tCls Loss 0.9131\tRank Loss 0.1902\tPrec 65.62% (53.09)%\n",
      "Epoch: 3\tTime 0.825 (3.178)\tData 0.002 (0.211)\tLoss 1.5168\tCls Loss 1.2326\tRank Loss 0.2257\tPrec 54.69% (53.21)%\n",
      "AURC 271.96\n",
      "EAURC 146.78\n",
      "AUPR 67.59\n",
      "FPR 80.92\n",
      "ECE 9.73 \n",
      "NLL 13.16 \n",
      "Brier 60.20\n",
      "Accuracy:54.339999999999996\n",
      "AURC 271.96\n",
      "EAURC 146.78\n",
      "AUPR 67.59\n",
      "FPR 80.92\n",
      "ECE 9.73 \n",
      "NLL 13.16 \n",
      "Brier 60.20\n",
      "Accuracy:54.339999999999996\n"
     ]
    }
   ],
   "source": [
    "main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b1b22-8ee0-4860-8e93-731bf1a74d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
